---
title: "HW1"
author: "Seth Russell"
date: "3/27/2018"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

library(foreign)
library(rms)
library(ggplot2)
library(pROC)
```

Note to self: Use http://www.stat.cmu.edu/~cshalizi/rmarkdown/ to figure out math symbols!

## 2.	Simulation, linear regression, variance explained, predictions and predictive accuracy

In this exercise you’ll create some simulated data from a linear model of a continuous outcome and will fit simple regression models to them. Make sure to set a seed prior to starting part (a) to ensure consistent results. Use base R and the rms package. 

### (a)	Using the rnorm() function, create a vector, x, containing 100 observations drawn from a N(0, 1) distribution. This represents a feature, X.

```{r 2a}
set.seed(1984)
x <- rnorm(100, 0, 1)
```


### (b)	Using the rnorm()function, create a vector, eps, containing 100 observations drawn from a N(0, 0.25) distribution i.e. a normal distribution with mean zero and variance 0.25.

```{r 2b}
set.seed(1776)
eps <- rnorm(100, 0, 0.25)

```

### (c)	Using x and eps, generate a vector y according to the model
Y = -1 + 0.5X  + $\epsilon$

```{r 2c}
y <- -1 + 0.5 * x + eps

```

### (d)	Create a scatterplot displaying the relationship between x and y. Comment on what you observe.

```{r 2d}
plot(x, y)
```

Result is what appears to be an approximately linear relationship between x and y

### (e)	Fit a least squares linear model to predict y using x. Comment on the model obtained. How do $\hat{\beta_0}$ and $\hat{\beta_1}$ compare to $\beta_0$ and  $\beta_1$ ?


```{r 2e}
g <- rms::ols(y ~ x, x=T, y=T, se.fit=T)
g
anova(g)
g_dd <- datadist(x, y)
options(datadist="g_dd")
summary(g)
AIC(g)
hist(g$residuals, breaks=14)
plot(g$residuals, x)
plot(x, y)
abline(g)
```

Line generated by least squares method goes through center of random data that has a linear shape. By visual inspection, some are close to the linear function, but most are spread evenly around the line.

As shown via the residual plot, the values are off from the regression line by apparently a random amount (doesn't seem to have a visual pattern) ranging from -0.6 to 0.8.

### (f)	Now fit a polynomial regression model that predicts y using x and x2. Is there evidence that the quadratic term improves the model fit? Explain your answer.

```{r 2f}
x2 = x^2
p <- rms::ols(y ~ x + x2, x=T, y=T, se.fit=T)
p
anova(p)
p_dd <- datadist(x, x2, y)
options(datadist="p_dd")
summary(p)
AIC(p)
#plot(p$residuals, x)
hist(residuals(p), breaks=14)
plot(fitted(p),residuals(p))
plot(x, y)
y_pred <- predict(p, data.frame(x, x2))
lines(sort(x), y_pred[order(x)], col = "red")
```

The p value for the x^2 term is 0.1270, so would not meet the standard 0.05 threshold. Also, the coeffiecient for the x^2 term is close to 0, so doesn't impact the fit line by much. Visually, the quadratic fit line isn't much better than the linear line; additionally, the histogram for the residuals are similar between the linear and polynomial equations. Despite the poor p value, low coefficient, and similar residuals between models, the AIC for the polynomial is slightly better than the AIC for the linear model (though neither is very good).

### (g)	Repeat (a)–(f) after modifying the data generation process in such a way that there is less noise in the data. Describe your results.

```{r 2g}
set.seed(1984)
x_ln <- rnorm(100, 0, 1)
set.seed(1776)
eps_ln <- rnorm(100, 0, 0.125)
y_ln <- -1 + 0.5 * x_ln + eps_ln
plot(x_ln, y_ln)
g_ln <- rms::ols(y_ln ~ x_ln, x=T, y=T, se.fit=T)
g_ln
anova(g_ln)
g_ln_dd <- datadist(x_ln, y_ln)
options(datadist="g_ln_dd")
summary(g_ln)
print('AIC:')
AIC(g_ln)
hist(g_ln$residuals, breaks=14)
plot(g_ln$residuals, x_ln)
plot(x_ln, y_ln)
abline(g_ln, col='red')

x2_ln = x_ln^2
p_ln <- rms::ols(y_ln ~ x_ln + x2_ln, x=T, y=T, se.fit=T)
p_ln
anova(p_ln)
p_ln_dd <- datadist(x_ln, x2_ln, y_ln)
options(datadist="p_ln_dd")
summary(p_ln)
print('AIC:')
AIC(p_ln)
#plot(p$residuals, x)
hist(residuals(p_ln), breaks=14)
plot(fitted(p_ln),residuals(p_ln))
plot(x_ln, y_ln)
y_pred <- predict(p_ln, data.frame(x_ln, x2_ln))
lines(sort(x_ln), y_pred[order(x_ln)], col = "red")

```

As with the noisier data set previously shown, the p value for the x^2 term does not meet the standard 0.05 threshold. In this case, the coeffiecient for the x^2 term is closer to 0 than before.

A visual comparison of the fit line and histogram appears that the linear and polynomial are still very similar. 

For this less noisy dataset, the AIC for the polynomial is still better than the linear, though again they are both very similar. In comparison to the original random dataset, the AICs for these models has improved.


### (h)	Produce two separate plots showing the prediction bands for individual predictions from each of the models. Comment on what you observe about the uncertainty of predictions across the two models. What would you expect if you had built in more (rather than less) noise in the data

```{r 2h}
q2df <- data.frame(x, y, x2, eps)
ggplot(q2df, aes(x=x, y=y)) +
  geom_point() + 
  stat_smooth(method = 'lm', formula = y ~ x)

ggplot(q2df, aes(x=x, y=y)) +
  geom_point() + 
  stat_smooth(method = 'lm', formula = y ~ poly(x,2))

q2df_ln <- data.frame(x_ln, y_ln, x2_ln, eps_ln)
ggplot(q2df, aes(x=x_ln, y=y_ln)) +
  geom_point() + 
  stat_smooth(method = 'lm', formula = y ~ x)

ggplot(q2df, aes(x=x_ln, y=y_ln)) +
  geom_point() + 
  stat_smooth(method = 'lm', formula = y ~ poly(x,2))

```

As shown by the 95% confidence interval around the regression line/polynomial, there is some uncertainty in the best location for the prediction. The ends of the prediction line have the largest confidence interval. In comparison to the original vs less noisy data set, the less noisy data set has a smaller confidence interval. If the data set were to be more noisy, I would expect a larger confidence interval around the regression line.

### (i)	Familiarize yourself with the rms package in R. Use the validate.ols function to produce an “optimism- corrected” R2 value for each of the two models above. Explain how this differs from producing a bootstrap distribution of the R2 values themselves.

```{r 2i}
# had to modify call of rms::ols and add x=TRUE and y=TRUE
print('validate g')
validate(g)
print('validate p')
validate(p)
```

According to the rms::validate.ols documentation, "The validate function does resampling validation of a multiple linear regression model... Uses resampling to estimate the optimism in various measures of predictive accuracy which include R^2" Alternatively a bootstrap distribution of R^2 would find via sampling with replacement the distribution of possible R^2 values from the model.


## 3.	Logistic regression, calibration and discrimination

An extensive data set was used to develop a prognostic model for 6-month unfavorable outcome (d.unfav) after moderate and severe TBI (d.gos = 3, 4). The strongest predictors were age, motor score and pupil reactivity. See “TBI Background Document.pdf” (in Canvas under Files -> Data) for the data dictionary.  Note: Truncated systolic blood pressure (d.sysbt) is also included in the dataset but does not appear on the list in the document. The data are from Steyerberg and are in SPSS format. The dataset can be found in Canvas under Files -> Data -> TBI.sav. 

### (a)	Using the rms package in R, fit a logistic regression model (binary outcome) with these 3 variables (age, d.motor and pupil.i) in the US data set (trial=75). First create 5 dummy variables for motor score and 2 for pupil reactivity. Be sure to investigate a potential nonlinear effect of age in the model.

```{r 3a}
wd <- getwd()
setwd("..")
parent <- getwd()

tbi <- read.spss(file = paste(parent,'TBI.sav', sep='/'), to.data.frame = TRUE)
print(paste(parent,'TBI.sav', sep='/'))
setwd(wd)

# filter dataset
tbi = tbi[tbi$trial == 'Tirilazad US', ]

# convert d.motor column to factor
tbi$d.motor <- as.factor(tbi$d.motor)

```

LRM model using built in factor method to create dummy variables

```{r lrma}

tbi_model <- lrm(d.unfav ~ age + d.motor + pupil.i, data=tbi, x=TRUE, y=TRUE)
tbi_model
AIC(tbi_model)
tbi_dd <- datadist(tbi)
options(datadist="tbi_dd")
summary(tbi_model)

plot(anova(tbi_model), what='proportion chisq') # relative importance
plot(Predict(tbi_model, fun=plogis)) # predicted values
#  
# penalty <- pentrace(mod1, penalty=c(0.5,1,2,3,4,6,8,12,16,24), maxit=25)
# mod1_pen <- update(mod1, penalty=penalty$penalty)
# effective.df(mod1_pen)
# mod1_pen


```

Alternative method with dummy encoding instead of factors

```{r lrmb}
# can use dummies package to create dummy encoded variables
#install.packages('dummies')
library(dummies)
d_tbi = dummy.data.frame(tbi, names = c("d.motor","pupil.i") , sep = "_")

# or model.matrix to create dummy encoded variables
#model.matrix(~tbi$d.motor)

d_model <- lrm(d.unfav ~ age + d.motor_1 + d.motor_2 + d.motor_3 + d.motor_4 + d.motor_6 + `pupil.i_no reactive pupils` + `pupil.i_one reactive`, data=d_tbi, x=TRUE, y=TRUE)
d_model

d_tbi_dd <- datadist(d_tbi)
options(datadist="d_tbi_dd")
summary(d_model)

plot(anova(d_model), what='proportion chisq') # relative importance
rms::validate(d_model, method="boot", B=500) # bootstrapped validation
my.calib <- rms::calibrate(d_model, method="boot", B=500) # model calibration
plot(my.calib, las=1)

```

```{r 3alinearity}
#library(splines)

plot(smooth.spline(tbi$age, tbi$d.unfav, df=4), type="l", ylim=c(0,1))
points(tbi$age, tbi$d.unfav)

ggplot(tbi, aes(y=tbi$d.unfav, x=tbi$age)) +
  geom_point() +
  geom_smooth()
```

By plotting age vs unfavarable outcome it appears that younger age (from minimum to ~ 25) has little to no effect on an unfavorable outcome. Beyond age 25 there is an increasing linear relationship between age and unfavorable outcome.

### (b)	 Obtain the predicted probability according to the model for each patient and plot these indicating values for favorable vs. unfavorable outcomes.  

```{r 3b}
tbi_dd <- datadist(tbi)
options(datadist="tbi_dd")

ggplot(Predict(tbi_model, age, d.motor, pupil.i, fun=plogis))
ggplot(Predict(tbi_model, age, pupil.i, d.motor, fun=plogis))

y_prob <- predict(tbi_model, tbi, type="fitted")
plot(tbi$d.unfav, y_prob)
```

### (c)	What is the estimated fraction of unfavorable outcomes in the analyzed subset of patients? Verify that this is as expected for these data. 

```{r 3c}
noquote((c('Fraction of unfavavorable outcomes in dataset: ', sum(tbi$d.unfav)/nrow(tbi))))

# start with the naive threshold of 0.5
y_hat <- ifelse(y_prob <= 0.5, 1, 0)
noquote((c('Predicted fraction of unfavavorable outcomes in dataset: ', sum(y_hat)/nrow(tbi))))


# use ROC curve to find better threshold
roc.tbi <- roc(tbi$d.unfav, y_prob)
## Best cutoff by closest to topleft method
coords.tbi <- coords(roc = roc.tbi , x= "best" , input = "threshold", best.method = "closest.topleft")
y_hat <- ifelse(y_prob <= coords.tbi[1], 1, 0)
noquote((c('Alternate: Predicted fraction of unfavavorable outcomes in dataset: ', sum(y_hat)/nrow(tbi))))
```

### (d)	Now, assess the discrimination of the model. Be sure to correct for the optimism of using the same data for fitting and assessing discrimination using the validate.lr function. 

```{r 3d}

rms::validate(tbi_model, method="boot", B=500) # bootstrapped validation
my.calib <- rms::calibrate(tbi_model, method="boot", B=500) # model calibration
plot(my.calib, las=1)

```

### (e)	 Summarize the results in (a)-(d). 

The model overpredicts an unfavorable outcome with the selected predictors. Although it is stated that the strongest predictors were age, motor score and pupil reactivity, the p value obtained via rms.lrm are low for some. Perhaps a re-evaluation of 'strongest predictors' would be appropriate or looking at methods other than rms.lrm could yield better results.

### (f)	What would be the next steps in terms of assessing generalizability of this model to patients in an international trial? 

Run the same model on the international observations (g below). Alternatively, find a new dataset with internal results and run on that.

### (g)	EXTRA CREDIT: Carry out the next steps in (f) using the analogous international observations in the dataset (trial=74). Summarize the results for this test dataset.

```{r 3g}

```